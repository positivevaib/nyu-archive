Vaibhav A Gadodia
Basic Algorithms - HW 2


Problem 3

CALLING-FUNCTION(heap H, valueType x, int k)										
1. global count <- 0											
2. boolean1, boolean2 <- false									
3. if H.root.value < x											
4. 	count++												
5.	if H.root.child1 != null and H.root.child1.value < x						
6.		boolean1 <- RECURSIVE-FUNCTION(H.root.child1, ++count, x, k)				
7.	if H.root.child2 != null and H.root.child2.value < x						
8.		boolean2 <- RECURSIVE-FUNCTION(H.root.child2, ++count, x, k)				
9. return boolean1 or boolean2 or count >= k								

RECURSIVE-FUNCTION(node N, int count, valueType x, int k)						
1. if count >= k											
2. 	return true											
3. boolean1, boolean2 <- false										
4. if N.child1 != null and N.child1.value < x								
5. 	boolean1 <- RECURSIVE-FUNCTION(N.child1, ++count, x, k)						
6. if N.child2 != null and N.child2.value < x								
7. 	boolean2 <- RECURSIVE-FUNCTION(N.child2, ++count, x, k)						
8. return boolean1 or boolean2									

Total cost of CALLING-FUNCTION = O(k)

RECURSIVE-FUNCTION will either never be called or be called (k-1) times in total. This is so because this function is only used to visit the first (k-1) nodes (not including the root) that have values less than x and the function is able to terminate once it visits the k-th node because of the global count variable that keeps track of all valid nodes that have been visited.


Problem 4

K-WAY-MERGE(int n, list L1, list L2, ... , list LK)					
1. H <- new heap											
2. sorted <- new list											
3. for all lists										
4. 	H.insert(list.min)										
5. 	list.deleteMin()									
6. for i = 1 to n										
7. 	if i != n											
8. 		sorted.insert(H.min)							
9. 		originalListRef <- H.min.orignialList						
10. 		H.deleteMin()											
11. 		H.insert(originalListRef.min)								
12. 		originalListRef.deleteMin()								
13. 	else												
14. 		sorted.insert(H.min)									
15. return sorted											

Total cost of K-WAY-MERGE = O(nlog(k))

The cost to build the heap H is O(log(k)) as it will have k elements (the minimum element from each original list). Also, the cost to delete and then replace the deleted minimum element from heap H is O(log(k)) and since, the deletion and insertion is performed n times, the total replacement cost is O(nlog(k)). Thus, the total cost is O(n(log(k))

This is accomplished by designing the algorithm so that each entry in heap H doesn't just have its own value but also the reference to the list that it originally belonged to. This allows the algorithm to not have to test the minimum elements of all k lists against each other to find the smallest among them.


Problem 5

Data Structure:

The data structure to use for this problem is a 2-3 tree where the values are stored at the leaf nodes in the order that they are inserted in the tree. So, for instance, if values 1, 7, 5 ..., 4 are inserted in that order, then the left-most leaf will have value 1, the next leaf, value 7 and so on, with the right most leaf having value 4. Here, the values can be repeated as the values themselves don't matter, rather it is the insertion order that matters. Furthermore, the internal nodes will have the total number of descendants in the subtree with themselves as the root. So, for instance, in an internal node that is the root of a subtree with 5 leaf nodes, the internal node will have value 5.

Algorithms:

i) To create a new list with one item is as simple as instantiating a new 2-3 tree and inserting that value in it, which takes constant time as it only involves the instantiation of a leaf node.

ii) To concatenate two lists in time O(log(n)), the algorithm to be used is the same as the join operation for ordinary 2-3 trees. This is so because much like the join operation, which, relied on the assumption of the two trees (ones being merged) being in the correct final order, the list concatenation doesn't need to re-order any leaf nodes in the final tree.

iii) To split the list into two lists with one containing the first k items and the other, the rest, the algorithm to be used is very similar to the split operation used for ordinary 2-3 trees. Much like that split operation, this algorithm will also involve going down to the k-th leaf node and deleting the path travelled, and finally joining the free standing trees from inside out. The only difference here will be the search procedure to find the k-th node and that will be achieved through the guide values in the internal nodes, which in this case don't have the max value of the subtree, but rather the total descendants in the subtree. As a consequence, the search algorithm will keep track of a global count variable (which will remember the number of leaf nodes in the left subtree that was not visited) and will only visit that child node whose descendent leaf nodes will be in the range that houses the k-th leaf. So, for instance, while traversing down the path, if the search algorithm is at an internal node, (needing to find the 7th leaf node), and the count variable is 4, that would mean that the search algorithm will have to travel down to the leftmost child node where the count + child.guide > k
This algorithm will have the same running time as the ordinary split operation as the only difference here is in the search operation but even there, the total operational calls are the same. Therefore, the algorithm will run in O(log(n)) time.

iv) Here, the algorithm will be similar to the search algorithm except that it would not return a boolean but rather the leaf value. Also, here, the algorithm will not compare the child node values as in the ordinary case, but would instead use a global count variable (mentioned in part iii above) and would compare the sum of count and child node guide to figure out its path. Therefore, the running time will be O(log(n)) much like the ordinary search operation.


Problem 6

